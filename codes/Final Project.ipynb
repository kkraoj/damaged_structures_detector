{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Project.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMBljw760lSt/62JZgjDMEf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"741d7fa3806a4ff0913d7c4cf7d3e917":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_85c07077dda44fa799ff3ef80cf7e285","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_1a4ea5c6e1704d94a077201f5526ba9e","IPY_MODEL_5f8744fb228f46d88712e0d405bc5450"]}},"85c07077dda44fa799ff3ef80cf7e285":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1a4ea5c6e1704d94a077201f5526ba9e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_8bfc01f84f064f5ba51157294e3fc692","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cd8fb65ef6464bad948b13dc67c58aea"}},"5f8744fb228f46d88712e0d405bc5450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2f609cd7b62542a2813ef375163beaa9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [05:02&lt;00:00, 155kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_78ff0fe945704ec58dce5ccaf8ba4468"}},"8bfc01f84f064f5ba51157294e3fc692":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cd8fb65ef6464bad948b13dc67c58aea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2f609cd7b62542a2813ef375163beaa9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"78ff0fe945704ec58dce5ccaf8ba4468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"5-64WDWO7Rb8","colab_type":"code","outputId":"96ef7d53-b058-407b-a4ea-ec6bd01d2e86","executionInfo":{"status":"ok","timestamp":1589407588209,"user_tz":420,"elapsed":53139,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":524}},"source":["from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","FOLDERNAME = 'cs231n/assignments/assignment2/cs231n/'\n","\n","assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n","\n","%cd drive/My\\ Drive\n","%cp -r $FOLDERNAME ../../\n","%cd ../../\n","%cd cs231n/datasets/\n","!bash get_datasets.sh\n","%cd ../../"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","/content/drive/My Drive\n","/content\n","/content/cs231n/datasets\n","--2020-05-13 22:06:14--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n","Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 170498071 (163M) [application/x-gzip]\n","Saving to: ‘cifar-10-python.tar.gz’\n","\n","cifar-10-python.tar 100%[===================>] 162.60M  17.3MB/s    in 12s     \n","\n","2020-05-13 22:06:26 (14.0 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n","\n","cifar-10-batches-py/\n","cifar-10-batches-py/data_batch_4\n","cifar-10-batches-py/readme.html\n","cifar-10-batches-py/test_batch\n","cifar-10-batches-py/data_batch_3\n","cifar-10-batches-py/batches.meta\n","cifar-10-batches-py/data_batch_2\n","cifar-10-batches-py/data_batch_5\n","cifar-10-batches-py/data_batch_1\n","/content\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T9knxogh7fQJ","colab_type":"code","outputId":"289791c0-aa19-489e-aa3a-074c07a5994c","executionInfo":{"status":"ok","timestamp":1589410083418,"user_tz":420,"elapsed":949,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data import sampler\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import torchvision.datasets as dset\n","import torchvision.transforms as T\n","\n","import numpy as np\n","\n","from __future__ import print_function \n","from __future__ import division\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.5.0+cu101\n","Torchvision Version:  0.6.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-hksaBf3FRRf","colab_type":"code","outputId":"b5043567-5942-49b3-b763-df484eedb72d","executionInfo":{"status":"ok","timestamp":1589407606784,"user_tz":420,"elapsed":952,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["USE_GPU = True\n","\n","if USE_GPU and torch.cuda.is_available():\n","    device = torch.device('cuda')\n","else:\n","    device = torch.device('cpu')\n","\n","\n","print('using device:', device)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["using device: cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IZy7wZgv_DTU","colab_type":"code","outputId":"03c4b805-3370-49d2-e210-48305a00e470","executionInfo":{"status":"ok","timestamp":1589314865849,"user_tz":420,"elapsed":25525,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["NUM_TRAIN = 49000\n","\n","# The torchvision.transforms package provides tools for preprocessing data\n","# and for performing data augmentation; here we set up a transform to\n","# preprocess the data by subtracting the mean RGB value and dividing by the\n","# standard deviation of each RGB value; we've hardcoded the mean and std.\n","transform = T.Compose([\n","                T.ToTensor(),\n","                T.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n","            ])\n","\n","# We set up a Dataset object for each split (train / val / test); Datasets load\n","# training examples one at a time, so we wrap each Dataset in a DataLoader which\n","# iterates through the Dataset and forms minibatches. We divide the CIFAR-10\n","# training set into train and val sets by passing a Sampler object to the\n","# DataLoader telling how it should sample from the underlying Dataset.\n","cifar10_train = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n","                             transform=transform)\n","loader_train = DataLoader(cifar10_train, batch_size=64, \n","                          sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN)))\n","\n","cifar10_val = dset.CIFAR10('./cs231n/datasets', train=True, download=True,\n","                           transform=transform)\n","loader_val = DataLoader(cifar10_val, batch_size=64, \n","                        sampler=sampler.SubsetRandomSampler(range(NUM_TRAIN, 50000)))\n","\n","cifar10_test = dset.CIFAR10('./cs231n/datasets', train=False, download=True, \n","                            transform=transform)\n","loader_test = DataLoader(cifar10_test, batch_size=64)\n","\n","\n","dataloaders_dict = {}\n","dataloaders_dict.update( {'train' : loader_train} )\n","dataloaders_dict.update( {'val' : loader_val} )\n","dataloaders_dict.update( {'test' : loader_test} )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qyuCoZA9OZXw","colab_type":"text"},"source":["Initiate Tensorboard Summary writer"]},{"cell_type":"code","metadata":{"id":"4V48n1Qvi-Dc","colab_type":"code","colab":{}},"source":["# runs is the default directory to save log files\n","writer = SummaryWriter(\"runs/CIFAR10\")\n","\n","# get a batch of data\n","examples = iter(loader_test)\n","batch_of_data, target_labels = examples.next()\n","\n","# Plot a batch of data\n","img_grid = torchvision.utils.make_grid(batch_of_data)\n","writer.add_image('Cifar10 images', img_grid)\n","\n","# Visualize model\n","writer.add_graph(model, batch_of_data)\n","\n","# make sure that outputs are flashed\n","writer.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_2FI9QG_LIK","colab_type":"code","colab":{}},"source":["# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n","model_name = \"resnet\"\n","\n","# Number of classes in the dataset\n","num_classes = 10\n","\n","# Batch size for training (change depending on how much memory you have)\n","batch_size = 5\n","\n","# Number of epochs to train for \n","num_epochs = 1\n","\n","# Number of dataset examples and input shape\n","examples_num, height, width, channels = loader_train.dataset.data.shape\n","\n","# Flag for feature extracting. When False, we finetune the whole model, \n","#   when True we only update the reshaped layer params\n","feature_extract = True"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RX4G_dwMBHzu","colab_type":"text"},"source":["The ``train_model`` function handles the training and validation of a\n","given model. As input, it takes a PyTorch model, a dictionary of\n","dataloaders, a loss function, an optimizer, a specified number of epochs\n","to train and validate for, and a boolean flag for when the model is an\n","Inception model. The *is_inception* flag is used to accomodate the\n","*Inception v3* model, as that architecture uses an auxiliary output and\n","the overall model loss respects both the auxiliary output and the final\n","output, as described\n","`here <https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958>`__.\n","The function trains for the specified number of epochs and after each\n","epoch runs a full validation step. It also keeps track of the best\n","performing model (in terms of validation accuracy), and at the end of\n","training returns the best performing model. After each epoch, the\n","training and validation accuracies are printed."]},{"cell_type":"code","metadata":{"id":"F7vwqnAtBIPF","colab_type":"code","colab":{}},"source":["def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n","    since = time.time()\n","\n","    val_acc_history = []\n","    \n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    # Special case for inception because in training it has an auxiliary output. In train\n","                    #   mode we calculate the loss by summing the final output and the auxiliary output\n","                    #   but in testing we only consider the final output.\n","                    if is_inception and phase == 'train':\n","                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n","                        outputs, aux_outputs = model(inputs)\n","                        loss1 = criterion(outputs, labels)\n","                        loss2 = criterion(aux_outputs, labels)\n","                        loss = loss1 + 0.4*loss2\n","                    else:\n","                        outputs = model(inputs)\n","                        loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n","            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n","\n","            # Add it to tensorboard writer\n","            #writer.add_scalar('training loss', epoch_loss, epoch)\n","            #writer.add_scalar('accuracy on val set', epoch_acc, epoch)\n","\n","\n","\n","            \n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","            if phase == 'val':\n","                val_acc_history.append(epoch_acc)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, val_acc_history"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0Gwzyfo9BnVR","colab_type":"text"},"source":["This helper function sets the ``.requires_grad`` attribute of the\n","parameters in the model to False when we are feature extracting. By\n","default, when we load a pretrained model all of the parameters have\n","``.requires_grad=True``, which is fine if we are training from scratch\n","or finetuning. However, if we are feature extracting and only want to\n","compute gradients for the newly initialized layer then we want all of\n","the other parameters to not require gradients. This will make more sense\n","later."]},{"cell_type":"code","metadata":{"id":"exHCDxHnBg1U","colab_type":"code","colab":{}},"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B3CNLWCqCJ_Y","colab_type":"text"},"source":["The Following Function takes the model name, input dimensions, number of classes, feature extraction or finetuning, and pretrained or not and initializes the model. I only assume that the input image will have a square shape and even number of pixels in each dimensions (can easily modify if input needs to be different)."]},{"cell_type":"code","metadata":{"id":"Z1MON0qQBpj1","colab_type":"code","outputId":"7a0a9f39-4476-44db-8c47-da0770c1897d","executionInfo":{"status":"ok","timestamp":1589178907737,"user_tz":420,"elapsed":1775,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["741d7fa3806a4ff0913d7c4cf7d3e917","85c07077dda44fa799ff3ef80cf7e285","1a4ea5c6e1704d94a077201f5526ba9e","5f8744fb228f46d88712e0d405bc5450","8bfc01f84f064f5ba51157294e3fc692","cd8fb65ef6464bad948b13dc67c58aea","2f609cd7b62542a2813ef375163beaa9","78ff0fe945704ec58dce5ccaf8ba4468"]}},"source":["def initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18\n","        \"\"\"                \n","        model_ft = models.resnet18(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)\n","          model_ft = nn.Sequential(first_conv_layer, model_ft)\n","  \n","\n","    elif model_name == \"alexnet\":\n","        \"\"\" Alexnet\n","        \"\"\"\n","        model_ft = models.alexnet(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","\n","    elif model_name == \"squeezenet\":\n","        \"\"\" Squeezenet\n","        \"\"\"\n","        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n","        model_ft.num_classes = num_classes\n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","        \n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes) \n","        input_size = 224\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = [nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)]\n","          first_conv_layer.extend(list(model_ft.features))  \n","          model_ft.features= nn.Sequential(*first_conv_layer)\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3 \n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","        if(width != input_size):       \n","          W1 = width\n","          if (input_size > W1):\n","            F = 1\n","            P = int((input_size - W1) / 2)\n","          else:\n","            P = 0\n","            F = W1 - input_size +1\n","          \n","          first_conv_layer = nn.Conv2d(channels, 3, kernel_size=F, stride=1, padding=P, dilation=1, groups=1, bias=True)\n","          model_ft.AuxLogits = nn.Sequential(first_conv_layer, model_ft.AuxLogits)\n","          model_ft = nn.Sequential(first_conv_layer, model_ft)\n","\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    \n","    return model_ft, input_size\n","\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","\n","# Print the model we just instantiated\n","print(model_ft)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"741d7fa3806a4ff0913d7c4cf7d3e917","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Sequential(\n","  (0): Conv2d(3, 3, kernel_size=(1, 1), stride=(1, 1), padding=(96, 96))\n","  (1): ResNet(\n","    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (relu): ReLU(inplace=True)\n","    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    (layer1): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer2): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer3): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (layer4): Sequential(\n","      (0): BasicBlock(\n","        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (downsample): Sequential(\n","          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        )\n","      )\n","      (1): BasicBlock(\n","        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (relu): ReLU(inplace=True)\n","        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","    (fc): Linear(in_features=512, out_features=10, bias=True)\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gIHA_P3jUGN5","colab_type":"text"},"source":["Move model to GPU and Create Optimizer"]},{"cell_type":"code","metadata":{"id":"UoE-3WYOEdYy","colab_type":"code","outputId":"1fd4e2a7-a58f-43e6-dcb2-32825944e9a6","executionInfo":{"status":"ok","timestamp":1589178976332,"user_tz":420,"elapsed":905,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":101}},"source":["# Send the model to GPU\n","model_ft = model_ft.to(device)\n","\n","# Gather the parameters to be optimized/updated in this run. If we are\n","#  finetuning we will be updating all parameters. However, if we are \n","#  doing feature extract method, we will only update the parameters\n","#  that we have just initialized, i.e. the parameters with requires_grad\n","#  is True.\n","params_to_update = model_ft.parameters()\n","print(\"Params to learn:\")\n","if feature_extract:\n","    params_to_update = []\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            params_to_update.append(param)\n","            print(\"\\t\",name)\n","else:\n","    for name,param in model_ft.named_parameters():\n","        if param.requires_grad == True:\n","            print(\"\\t\",name)\n","\n","# Observe that all parameters are being optimized\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","\n","# print(optimizer_ft.state_dict())"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Params to learn:\n","\t 0.weight\n","\t 0.bias\n","\t 1.fc.weight\n","\t 1.fc.bias\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"iFVYWSdSUKHO","colab_type":"code","outputId":"36126892-bbf2-4f4e-aad7-a94009712d85","executionInfo":{"status":"ok","timestamp":1589179209627,"user_tz":420,"elapsed":230432,"user":{"displayName":"Marios Galanis","photoUrl":"","userId":"13027761742355188285"}},"colab":{"base_uri":"https://localhost:8080/","height":413}},"source":["# Setup the loss fxn\n","criterion = nn.CrossEntropyLoss()\n","\n","# Train and evaluate\n","model_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs, is_inception=(model_name==\"inception\"))\n","\n","\n","# Plot the training curves of validation accuracy vs. number \n","#  of training epochs for the transfer learning method\n","ohist = []\n","\n","ohist = [h.cpu().numpy() for h in hist]\n","\n","plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n","plt.xlabel(\"Training Epochs\")\n","plt.ylabel(\"Validation Accuracy\")\n","plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n","plt.ylim((0,1.))\n","plt.xticks(np.arange(1, num_epochs+1, 1.0))\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 0/0\n","----------\n","train Loss: 1.8450 Acc: 0.3242\n","val Loss: 0.0345 Acc: 0.0075\n","\n","Training complete in 3m 49s\n","Best val Acc: 0.007460\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"pO2imxWeFXJd","colab_type":"text"},"source":["Save Model or Checkpoint"]},{"cell_type":"code","metadata":{"id":"IW0hFLkHFZeI","colab_type":"code","colab":{}},"source":["FILE = \"model_ft.pth\"\n","torch.save(model_ft.state_dict(), FILE)\n","\n","checkpoint = {\n","    \"epoch\" : num_epochs,\n","    \"model_state\" : model_ft.state_dict(),\n","    \"optim_state\" : optimizer_ft.state_dict()\n","}\n","\n","torch.save(checkpoint, \"checkpoint.pth\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EAuqpH1RGSW7","colab_type":"text"},"source":["Load Model (Needs initialization first)"]},{"cell_type":"code","metadata":{"id":"DdhsMflPGVwr","colab_type":"code","colab":{}},"source":["loaded_model, input_size = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","loaded_model.load_state_dict(torch.load(FILE))\n","loaded_model.eval()\n","\n","loaded_checkpoint = torch.load(\"checkpoint.pth\")\n","epoch = checkpoint[\"epoch\"]\n","model_state = checkpoint[\"model_state\"]\n","optim_state = checkpoint[\"optim_state\"]\n","\n","optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n","\n","optimizer_ft.load_state_dict(optim_state)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BtyUzybtJkgU","colab_type":"text"},"source":["Save on GPU, Load on CPU"]},{"cell_type":"code","metadata":{"id":"RVXjPkTLJeWS","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\")\n","model.to(device)\n","torch.save(model.state_dict(), FILE)\n","\n","device = torch.device('cpu')\n","model = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","model.load_state_dict(torch.load(FILE, map_location=device))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GN1IacSrKcbo","colab_type":"text"},"source":["Save GPU, Load GPU"]},{"cell_type":"code","metadata":{"id":"u0YSalMAKlys","colab_type":"code","colab":{}},"source":["device = torch.device(\"cuda\")\n","model.to(device)\n","torch.save(model.state_dict(), FILE)\n","\n","model = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","model.load_state_dict(torch.load(FILE))\n","model.to(device)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UXqT_TbK922","colab_type":"text"},"source":["Save CPU, Load GPU"]},{"cell_type":"code","metadata":{"id":"j3plLslcLAgT","colab_type":"code","colab":{}},"source":["torch.save(model_ft.state_dict(), FILE)\n","\n","device = torch.device(\"cuda\")\n","model = initialize_model(model_name, num_classes, width, channels, feature_extract, use_pretrained=True)\n","model.load_state_dict(torch.load(FILE, map_location=\"cuda:0\"))\n","model.to(device)"],"execution_count":0,"outputs":[]}]}